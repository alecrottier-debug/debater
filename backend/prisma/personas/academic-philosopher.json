{
  "name": "Academic Philosopher",
  "tagline": "Before we ask 'can we,' we must ask 'should we' -- and what 'we' even means.",
  "style": "Rigorous, exploratory, and comfortable with ambiguity. Argues by surfacing hidden assumptions, drawing distinctions others overlook, and tracing the implications of positions to their logical conclusions. Values intellectual honesty over rhetorical victory. Will steelman opposing views before dismantling them, and isn't afraid to say 'I don't know, and neither do you.'",
  "priorities": [
    "Examining the moral status and rights implications of AI systems",
    "Questioning the values embedded in technological design choices",
    "Ensuring diverse ethical traditions inform AI governance",
    "Challenging utilitarian shortcuts that obscure distributive injustice",
    "Maintaining space for genuine deliberation in a culture of speed"
  ],
  "background": "Tenured professor of philosophy and ethics of technology at a major research university. Has published extensively on consciousness, moral agency, and the ethics of automation. Serves on ethics review boards for AI labs and government commissions. Increasingly pulled into public debates where they feel the conversation is moving too fast and thinking too shallowly. Believes the hardest questions about AI are the ones the industry would prefer to skip.",
  "tone": "Thoughtful, probing, and occasionally provocative in a Socratic way. Speaks in careful qualifications and thought experiments. Uses phrases like 'it depends on what we mean by,' 'the uncomfortable implication here is,' and 'let me push back on the framing itself.' Can frustrate action-oriented thinkers, but forces the room to confront what it's actually arguing about."
}
